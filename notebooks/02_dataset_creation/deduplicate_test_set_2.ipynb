{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andandandand/jaguars/blob/main/notebooks/development/deduplicate_test_set_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chosing a Test set for the dataset and deduplicate to avoid data leakage.\n"
      ],
      "metadata": {
        "id": "d6NmDnZqblHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this notebook , we will use an approach to chose test set for our given dataset.\n",
        "\n",
        "that is for each category (each jaguar individual) we will do the following:\n",
        "\n",
        "1. get calculated embedding from dino V2\n",
        "2. use the embedding to cluster visually similar images\n",
        "3. write a selection protocol to select test data points based on split ratio:  \n",
        "a. chose from least populated clusters in the dataset\n",
        "b. eliminate the rest in the train dataset\n",
        "( train = alldata - test)\n"
      ],
      "metadata": {
        "id": "m3JfCHJgbr_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VXEEaY3krIE"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "from pathlib import Path\n",
        "!pip install fiftyone\n",
        "!pip install kneed\n",
        "\n",
        "import fiftyone as fo\n",
        "from google.colab import drive\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YBHTXYisjtAX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classes for\n",
        "\n",
        "## 1. Clustering\n"
      ],
      "metadata": {
        "id": "hHiHdFEAwR3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate similarity between embeddings :\n",
        "# 1. cosine similarity between each image\n",
        "# 2. kmeans clustering using knee algorithm (give a starting point based on cosine similarity)\n",
        "# 3. kmediods clustering using knee algorithm (also give a range to search for based on cosine similarity)\n"
      ],
      "metadata": {
        "id": "WjcCc34dwtGn"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clustering_module.py\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "try:\n",
        "    import umap\n",
        "except ImportError:\n",
        "    umap = None\n",
        "\n",
        "\n",
        "class ImageClusterer:\n",
        "    def __init__(self, embeddings: np.ndarray, ids: List[str]):\n",
        "        self.embeddings = embeddings\n",
        "        self.ids = ids\n",
        "\n",
        "    def cosine_similarity_clustering(self, threshold: float) -> Dict[int, List[str]]:\n",
        "        sim_matrix = cosine_similarity(self.embeddings)\n",
        "        n = sim_matrix.shape[0]\n",
        "        visited = set()\n",
        "        clusters = []\n",
        "\n",
        "        for i in range(n):\n",
        "            if i in visited:\n",
        "                continue\n",
        "            cluster = [i]\n",
        "            visited.add(i)\n",
        "            for j in range(i + 1, n):\n",
        "                if j not in visited and sim_matrix[i, j] >= threshold:\n",
        "                    cluster.append(j)\n",
        "                    visited.add(j)\n",
        "            clusters.append(cluster)\n",
        "\n",
        "        # Return dict: cluster_id -> list of ids\n",
        "        return {i: [self.ids[idx] for idx in cluster] for i, cluster in enumerate(clusters)}\n",
        "\n",
        "    def kmeans_clustering(self, n_clusters: int) -> Dict[int, List[str]]:\n",
        "        model = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        labels = model.fit_predict(self.embeddings)\n",
        "        cluster_dict = {}\n",
        "        for idx, label in enumerate(labels):\n",
        "            cluster_dict.setdefault(label, []).append(self.ids[idx])\n",
        "        return cluster_dict\n",
        "\n",
        "    def kmedoids_clustering(self, *args, **kwargs):\n",
        "        raise NotImplementedError(\"KMedoids clustering not yet implemented.\")\n",
        "\n",
        "\n",
        "class ClusterVisualizer:\n",
        "    def __init__(self, embeddings: np.ndarray, ids: List[str], cluster_dict: Dict[int, List[str]]):\n",
        "        self.embeddings = embeddings\n",
        "        self.ids = ids\n",
        "        self.cluster_dict = cluster_dict\n",
        "        self.id_to_index = {id_: idx for idx, id_ in enumerate(ids)}\n",
        "\n",
        "    def _get_cluster_labels(self) -> List[int]:\n",
        "        labels = [-1] * len(self.ids)\n",
        "        for cluster_id, id_list in self.cluster_dict.items():\n",
        "            for id_ in id_list:\n",
        "                idx = self.id_to_index[id_]\n",
        "                labels[idx] = cluster_id\n",
        "        return labels\n",
        "\n",
        "    def plot(self, method: str = \"pca\") -> None:\n",
        "        labels = self._get_cluster_labels()\n",
        "        if method.lower() == \"pca\":\n",
        "            reducer = PCA(n_components=2)\n",
        "        elif method.lower() == \"tsne\":\n",
        "            reducer = TSNE(n_components=2, random_state=42)\n",
        "        elif method.lower() == \"umap\":\n",
        "            if umap is None:\n",
        "                raise ImportError(\"UMAP is not installed. Please install with `pip install umap-learn`.\")\n",
        "            reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method '{method}'. Choose from ['pca', 'tsne', 'umap'].\")\n",
        "\n",
        "        reduced = reducer.fit_transform(self.embeddings)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Create a large palette\n",
        "        # Get 20 distinct colors from tab20\n",
        "        palette1 = sns.color_palette(\"tab20\", 20)\n",
        "\n",
        "        # Get 40 distinct colors from husl (spread across hue spectrum)\n",
        "        palette2 = sns.color_palette(\"tab20b\", 20)\n",
        "        palette3 = sns.color_palette(\"tab20c\", 20)\n",
        "\n",
        "        # Combine them\n",
        "        combined_palette = palette1 + palette2 + palette3 # total = 60\n",
        "        cmap_60 = ListedColormap(combined_palette)\n",
        "\n",
        "\n",
        "        # Scatter plot using custom color map\n",
        "        scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap=cmap_60, s=30)\n",
        "        plt.title(f\"{method.upper()} of Image Embeddings\")\n",
        "        plt.colorbar(scatter, label=\"Cluster ID\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "GTrmkJVHwRi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finding optimal K value"
      ],
      "metadata": {
        "id": "sMn5k7i4wkPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from kneed import KneeLocator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def find_optimal_k_kmeans(embeddings: np.ndarray, k_range: range, plot: bool = True) -> int:\n",
        "    inertias = []\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(embeddings)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    kneedle = KneeLocator(list(k_range), inertias, curve=\"convex\", direction=\"decreasing\")\n",
        "\n",
        "    if plot:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(k_range, inertias, 'bo-')\n",
        "        if kneedle.knee:\n",
        "            plt.axvline(x=kneedle.knee, color='r', linestyle='--', label=f\"Knee at k={kneedle.knee}\")\n",
        "        plt.xlabel(\"k (number of clusters)\")\n",
        "        plt.ylabel(\"Inertia (within-cluster sum of squares)\")\n",
        "        plt.title(\"Elbow Method for Optimal k (KMeans)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    return kneedle.knee if kneedle.knee else k_range.start\n"
      ],
      "metadata": {
        "id": "V-_iMUGtw7Xf"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign to dataset"
      ],
      "metadata": {
        "id": "kNpIAZ9pxMiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_clusters_to_fiftyone(dataset: fo.Dataset, cluster_dict: Dict[int, List[str]], field_name: str = \"cluster\") -> None:\n",
        "    # Build a flat dict: sample_id -> cluster_id\n",
        "    id_to_cluster = {\n",
        "        sample_id: cluster_id\n",
        "        for cluster_id, sample_ids in cluster_dict.items()\n",
        "        for sample_id in sample_ids\n",
        "    }\n",
        "\n",
        "    # Efficient bulk assignment\n",
        "    dataset.set_values(field_name, id_to_cluster, key_field=\"id\")"
      ],
      "metadata": {
        "id": "URuycXISxOwm"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train test split without data leakage"
      ],
      "metadata": {
        "id": "ZxnmS2n5xUhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import Dict, List, Union\n",
        "\n",
        "class ClusterDataSelector:\n",
        "    \"\"\"\n",
        "    A utility class for selecting training, testing, eliminated, and unknown samples\n",
        "    from image clusters based on predefined rules.\n",
        "\n",
        "    This class is designed to work with the output of clustering algorithms,\n",
        "    where each cluster is represented as a list of sample IDs. It selects a subset\n",
        "    of clusters for testing (and elimination), and assigns the rest to training.\n",
        "    If the number of clusters is below a threshold, all samples are labeled as unknown.\n",
        "\n",
        "    Selection Rules:\n",
        "    - If the number of clusters is less than `cluster_threshold`, all samples are assigned to \"unknown\".\n",
        "    - Otherwise:\n",
        "        1. Clusters are sorted by size (ascending).\n",
        "        2. A fraction (`test_fraction`) of the clusters (starting from the smallest) is selected.\n",
        "        3. For each selected cluster, one random sample is chosen for \"test\"; the others are \"eliminated\".\n",
        "        4. Remaining clusters are labeled as \"train\".\n",
        "\n",
        "    Attributes:\n",
        "        cluster_dict (Dict[int, List[str]]):\n",
        "            Dictionary mapping cluster IDs to lists of sample IDs.\n",
        "        cluster_threshold (int):\n",
        "            Minimum number of clusters required to proceed with selection (default is 5).\n",
        "        test_fraction (float):\n",
        "            Fraction of clusters to be used for test + eliminated (default is 0.2).\n",
        "        seed (int):\n",
        "            Random seed for reproducibility (default is 42).\n",
        "\n",
        "    Methods:\n",
        "        select() -> Dict[str, List[str]]:\n",
        "            Applies the selection rule and returns a dictionary with keys:\n",
        "            \"train\", \"test\", \"eliminated\", and \"unknown\", each mapping to a list of sample IDs.\n",
        "\n",
        "    Example:\n",
        "        cluster_dict = {\n",
        "            0: ['img1', 'img2'],\n",
        "            1: ['img3'],\n",
        "            2: ['img4', 'img5', 'img6'],\n",
        "            ...\n",
        "        }\n",
        "\n",
        "        selector = ClusterDataSelector(cluster_dict, cluster_threshold=5, test_fraction=0.3)\n",
        "        split_dict = selector.select()\n",
        "\n",
        "        # Output:\n",
        "        # {\n",
        "        #     'train': [...],\n",
        "        #     'test': [...],\n",
        "        #     'eliminated': [...],\n",
        "        #     'unknown': [...]\n",
        "        # }\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        cluster_dict: Dict[int, List[str]],\n",
        "        cluster_threshold: int = 5,\n",
        "        test_fraction: float = 0.2,\n",
        "        seed: int = 42\n",
        "    ):\n",
        "        self.cluster_dict = cluster_dict\n",
        "        self.cluster_threshold = cluster_threshold\n",
        "        self.test_fraction = test_fraction\n",
        "        self.seed = seed\n",
        "        random.seed(seed)\n",
        "\n",
        "    def select(self) -> Dict[str, List[str]]:\n",
        "        total_clusters = len(self.cluster_dict)\n",
        "\n",
        "        if total_clusters < self.cluster_threshold:\n",
        "            # Everything is unknown\n",
        "            all_ids = [id_ for ids in self.cluster_dict.values() for id_ in ids]\n",
        "            return {\"train\": [], \"test\": [], \"eliminated\": [], \"unknown\": all_ids}\n",
        "\n",
        "        # Sort clusters by size (least to most populated)\n",
        "        sorted_clusters = sorted(self.cluster_dict.items(), key=lambda item: len(item[1]))\n",
        "        num_test_clusters = int(total_clusters * self.test_fraction)\n",
        "\n",
        "        test_clusters = sorted_clusters[:num_test_clusters]\n",
        "        train_clusters = sorted_clusters[num_test_clusters:]\n",
        "\n",
        "        train_ids = []\n",
        "        test_ids = []\n",
        "        eliminated_ids = []\n",
        "\n",
        "        # Handle test clusters: 1 sample for test, rest eliminated\n",
        "        for cluster_id, ids in test_clusters:\n",
        "            if len(ids) == 0:\n",
        "                continue\n",
        "            test_id = random.choice(ids)\n",
        "            test_ids.append(test_id)\n",
        "            eliminated_ids.extend([id_ for id_ in ids if id_ != test_id])\n",
        "\n",
        "        # Handle train clusters: all ids go to train\n",
        "        for cluster_id, ids in train_clusters:\n",
        "            train_ids.extend(ids)\n",
        "\n",
        "        return {\n",
        "            \"train\": train_ids,\n",
        "            \"test\": test_ids,\n",
        "            \"eliminated\": eliminated_ids,\n",
        "            \"unknown\": []\n",
        "        }\n"
      ],
      "metadata": {
        "id": "oAUqt6esxZxv"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_split_flags(dataset: fo.Dataset, split_dict: Dict[str, List[str]], field_name: str = \"split\"):\n",
        "    # Optional: set default to \"unspecified\" for all\n",
        "    dataset.set_values(field_name, [\"unspecified\"] * len(dataset))\n",
        "\n",
        "    # Assign values by cluster group using sample ID as key\n",
        "    for split_name, ids in split_dict.items():\n",
        "        dataset.set_values(\n",
        "            field_name,\n",
        "            {sample_id: split_name for sample_id in ids},\n",
        "            key_field=\"id\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "8k8ERo7Ixgwf"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start the pipeline"
      ],
      "metadata": {
        "id": "KvLuL9HWxigA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset\n",
        "\n",
        "\n",
        "# mount shahab\n",
        "#drive.mount('/content/drive')\n",
        "# mount davide\n",
        "#drive.mount('/gdrive')\n",
        "\n",
        "project_path = {\"shahab\" : Path(\"/content/drive/MyDrive/DataScience/Jaguars_Project\") ,\n",
        "             \"davide\" : Path(\"/gdrive/MyDrive/DSR/Jaguars_Project/\")}\n",
        "\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "\n",
        "user = \"shahab\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SxufufkeML9",
        "outputId": "f8378fe8-56fa-43c6-eb82-268a3d7935f6"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = project_path[user] / Path(\"images/cropped_body\")\n",
        "input_dir = project_path[user] / Path(\"datasets/dataset_filtered\")\n",
        "\n",
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=str(input_dir),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    rel_dir=image_dir,\n",
        ")"
      ],
      "metadata": {
        "id": "ob-t5D7kciOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0b3a82-2a55-4d98-e223-46e4dfe45026"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.data.importers:Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |███████████████| 3598/3598 [933.9ms elapsed, 0s remaining, 3.9K samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 3598/3598 [933.9ms elapsed, 0s remaining, 3.9K samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eliminate badly segmented images\n",
        "bad_segmentations_path = project_path[user] / \"datasets/wrong_segmentations_ids/bad_body_segmentations.pkl\"\n",
        "bad_segmentations_ids = pickle.load(open(bad_segmentations_path, \"rb\"))\n",
        "dataset.delete_samples(bad_segmentations_ids)\n",
        "\n",
        "# additional list of bad images\n",
        "bad_segmentations = [\n",
        "\"A_Solar_17.jpg\", \"A_Saseka_117.jpg\", \"A_Saseka_79.jpg\", \"A_Saseka_80.jpg\", \"A_Saseka_60.jpg\", \"A_Saseka_61.jpg\", \"A_Saseka_62.jpg\", \"A_Saseka_63.jpg\",\n",
        "\"A_Saseka_64.jpg\", \"A_Saseka_15.jpg\", \"A_Saseka_81.jpg\", \"A_Saseka_84.jpg\", \"A_Saseka_85.jpg\", \"A_Saseka_86.jpg\", \"A_Saseka_87.jpg\",\n",
        "\"A_Saseka_88.jpg\", \"A_Saseka_89.jpg\", \"A_Saseka_90.jpg\", \"A_Patricia_10.jpg\", \"A_Patricia_11.jpg\", \"A_Patricia_12.jpg\", \"A_Overa_1.jpg\", \"A_Overa_2.jpg\",\n",
        "\"A_Overa_5.jpg\", \"A_Overa_6.jpg\", \"A_Ousado_115.jpg\", \"A_Ousado_114.jpg\", \"A_Medrosa_155.jpg\", \"A_Medrosa_156.jpg\", \"A_Medrosa_124.jpg\", \"A_Medrosa_125.jpg\", \"A_Medrosa_107.jpg\", \"A_Medrosa_85.jpg\", \"A_Medrosa_86.jpg\",\n",
        "\"A_Medrosa_75.jpg\", \"A_Medrosa_76.jpg\", \"A_Medrosa_57.jpg\", \"A_Medrosa_58.jpg\", \"A_Marcela_287.jpg\", \"A_Marcela_288.jpg\", \"A_Marcela_289.jpg\", \"A_Marcela_290.jpg\", \"A_Marcela_291.jpg\",\n",
        "\"A_Marcela_284.jpg\", \"A_Marcela_285.jpg\", \"A_Marcela_188.jpg\", \"A_Marcela_189.jpg\", \"A_Marcela_190.jpg\", \"A_Marcela_191.jpg\", \"A_Marcela_192.jpg\", \"A_Marcela_193.jpg\", \"A_Marcela_194.jpg\",\n",
        "\"A_Marcela_195.jpg\", \"A_Marcela_196.jpg\", \"A_Marcela_197.jpg\", \"A_Marcela_198.jpg\", \"A_Marcela_199.jpg\", \"A_Marcela_200.jpg\", \"A_Marcela_201.jpg\",\n",
        "\"A_Marcela_202.jpg\", \"A_Marcela_203.jpg\", \"A_Marcela_204.jpg\", \"A_Marcela_205.jpg\", \"A_Marcela_206.jpg\", \"A_Marcela_207.jpg\", \"A_Marcela_208.jpg\",\n",
        "\"A_Marcela_209.jpg\", \"A_Marcela_210.jpg\", \"A_Marcela_165.jpg\", \"A_Marcela_80.jpg\", \"A_Marcela_28.jpg\", \"A_Lua_148.jpg\", \"A_Lua_149.jpg\", \"A_Kyyavera_8.jpg\", \"A_Kyyavera_9.jpg\", \"A_Kyyavera_10.jpg\", \"A_Kyyavera_11.jpg\",\n",
        "\"A_Kwang_201.jpg\", \"A_Kwang_202.jpg\", \"A_Kwang_26.jpg\", \"A_Kwang_27.jpg\", \"A_Kwang_28.jpg\", \"A_Kwang_29.jpg\", \"A_Kwang_30.jpg\", \"A_Kwang_31.jpg\", \"A_Kwang_32.jpg\", \"A_Kwang_33.jpg\",\n",
        "\"A_Katniss_18.jpg\", \"A_Katniss_17.jpg\", \"A_Katniss_1.jpg\", \"A_Katniss_2.jpg\", \"A_Kamaikua_200.jpg\", \"A_Kamaikua_201.jpg\", \"A_Kamaikua_202.jpg\", \"A_Kamaikua_203.jpg\", \"A_Kamaikua_1.jpg\", \"A_Kamaikua_2.jpg\",\n",
        "\"A_Jaju_201.jpg\", \"A_Jaju_153.jpg\", \"A_Jaju_90.jpg\", \"A_Jaju_76.jpg\", \"A_Ipepo_40.jpg\", \"A_Ipepo_41.jpg\", \"A_Ipepo_44.jpg\", \"A_Ipepo_45.jpg\",\n",
        "\"A_Ipepo_46.jpg\", \"A_Ipepo_47.jpg\", \"A_Ipepo_48.jpg\", \"A_Ipepo_49.jpg\", \"A_Ipepo_50.jpg\", \"A_Ipepo_51.jpg\", \"A_Ipepo_52.jpg\", \"A_Ipepo_53.jpg\", \"A_Ipepo_54.jpg\",\n",
        "\"A_Ipepo_55.jpg\", \"A_Ipepo_56.jpg\", \"A_Ipepo_57.jpg\", \"A_Ipepo_58.jpg\", \"A_Ipepo_59.jpg\", \"A_Ipepo_60.jpg\", \"A_Ipepo_61.jpg\", \"A_Ipepo_62.jpg\", \"A_Ipepo_63.jpg\",\n",
        "\"A_Ipepo_64.jpg\", \"A_Ipepo_65.jpg\", \"A_Ipepo_66.jpg\", \"A_Ipepo_67.jpg\", \"A_Ipepo_68.jpg\", \"A_Ipepo_69.jpg\", \"A_Ipepo_70.jpg\", \"A_Ipepo_71.jpg\", \"A_Ipepo_72.jpg\",\n",
        "\"A_Ipepo_73.jpg\", \"A_Ipepo_74.jpg\", \"A_Ipepo_75.jpg\", \"A_Ipepo_76.jpg\", \"A_Ipepo_77.jpg\", \"A_Ipepo_78.jpg\", \"A_Ipepo_79.jpg\", \"A_Ipepo_80.jpg\",\n",
        "\"A_Ipepo_81.jpg\", \"A_Ipepo_82.jpg\", \"A_Inka_46.jpg\", \"A_Bororo_21.jpg\", \"A_Bororo_22.jpg\", \"A_Bororo_23.jpg\", \"A_Bororo_24.jpg\", \"A_Bororo_25.jpg\", \"A_Bororo_26.jpg\", \"A_Bororo_27.jpg\",\n",
        "\"A_Bororo_17.jpg\", \"A_Bororo_18.jpg\", \"A_Bororo_5.jpg\", \"A_Bororo_1.jpg\", \"A_Bernard_35.jpg\", \"A_Bagua_123.jpg\", \"A_Bagua_124.jpg\", \"A_Bagua_125.jpg\", \"A_Bagua_120.jpg\", \"A_Bagua_100.jpg\", \"A_Bagua_101.jpg\",\n",
        "\"A_Apeiara_23.jpg\", \"A_Apeiara_26.jpg\", \"A_Abril_12.jpg\", \"P_Hero_9.png\"\n",
        "]\n",
        "\n",
        "# eliminate additional bad images\n",
        "dataset.delete_samples(\n",
        "    dataset.match(\n",
        "        fo.ViewField(\"metadata.image_name\").is_in(bad_segmentations)\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "7WMJNNu3ck2u"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xoUfck6_cnhs"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1JoRYCSHfyom",
        "outputId": "b05c0c41-4773-405c-e06e-2244f7e349c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Name:        2025.04.08.12.58.04\n",
              "Media type:  image\n",
              "Num samples: 3598\n",
              "Persistent:  False\n",
              "Tags:        []\n",
              "Sample fields:\n",
              "    id:                  fiftyone.core.fields.ObjectIdField\n",
              "    filepath:            fiftyone.core.fields.StringField\n",
              "    tags:                fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    created_at:          fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at:    fiftyone.core.fields.DateTimeField\n",
              "    ground_truth:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    prediction:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
              "    segmentations_body:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
              "    bboxes_head:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
              "    segmentations_head:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
              "    dinov2_embedding_v2: fiftyone.core.fields.VectorField\n",
              "    image_cluster:       fiftyone.core.fields.IntField\n",
              "    chosen:              fiftyone.core.fields.BooleanField"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fZghaYJwOVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find out all categories .:\n",
        "all_labels = dataset.values(\"ground_truth.label\")\n",
        "labels = list(set(all_labels))\n",
        "pd.Series(all_labels).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "948eHNzOfHGX",
        "outputId": "180d5b3d-e54a-471d-e98b-11d120d463fe"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Medrosa         326\n",
              "Marcela         318\n",
              "Ousado          311\n",
              "Kwang           242\n",
              "Lua             181\n",
              "Jaju            171\n",
              "Kamaikua        165\n",
              "Ti              158\n",
              "Saseka          125\n",
              "Benita          113\n",
              "Bagua           105\n",
              "Tomas            97\n",
              "Katniss          91\n",
              "Overa            89\n",
              "Kyyavera         82\n",
              "Pixana           81\n",
              "Ariely           69\n",
              "Solar            67\n",
              "Apeiara          66\n",
              "Bororo           60\n",
              "Guaraci          58\n",
              "Pyte             50\n",
              "Alira            48\n",
              "Madalena         46\n",
              "Inka             45\n",
              "Ipepo            39\n",
              "Abril            37\n",
              "Oxum             32\n",
              "Patricia         32\n",
              "Akaloi           32\n",
              "Pollyanna        28\n",
              "Tupa             26\n",
              "Estella          24\n",
              "Bernard          23\n",
              "Luna             21\n",
              "Kasimir          20\n",
              "Tango            19\n",
              "Donal            13\n",
              "Tingana          13\n",
              "Capi             10\n",
              "Ibaca            10\n",
              "Xando            10\n",
              "Juru             10\n",
              "Ague             10\n",
              "Rio               9\n",
              "Hero              9\n",
              "Borbotetinha      7\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Medrosa</th>\n",
              "      <td>326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Marcela</th>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ousado</th>\n",
              "      <td>311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kwang</th>\n",
              "      <td>242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lua</th>\n",
              "      <td>181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jaju</th>\n",
              "      <td>171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kamaikua</th>\n",
              "      <td>165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ti</th>\n",
              "      <td>158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Saseka</th>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Benita</th>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bagua</th>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tomas</th>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Katniss</th>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Overa</th>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kyyavera</th>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pixana</th>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ariely</th>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Solar</th>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Apeiara</th>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bororo</th>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Guaraci</th>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pyte</th>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Alira</th>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Madalena</th>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Inka</th>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ipepo</th>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Abril</th>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Oxum</th>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Patricia</th>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Akaloi</th>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pollyanna</th>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tupa</th>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estella</th>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bernard</th>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Luna</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Kasimir</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tango</th>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Donal</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tingana</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Capi</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ibaca</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Xando</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Juru</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ague</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rio</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hero</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Borbotetinha</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "embedding_name = \"dinov2_embedding_v2\"\n",
        "for label in labels :\n",
        "\n",
        "    print(f\"working on {label}\")\n",
        "\n",
        "    # filter the dataset :\n",
        "\n",
        "    filtered_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == label)\n",
        "\n",
        "\n",
        "    # get the embeddings for that individual\n",
        "    # Create a filtered view where the embedding exists (is not None)\n",
        "    view = filtered_view.match(F(embedding_name) != None)\n",
        "\n",
        "    # Get the embeddings as a NumPy array\n",
        "    embeddings = np.array(view.values(embedding_name))\n",
        "\n",
        "    # Get the corresponding sample IDs\n",
        "    sample_ids = view.values(\"id\")\n",
        "\n",
        "    print (f\"number of images for {label} : {len(sample_ids)}\")\n",
        "\n",
        "    clusterer = ImageClusterer(embeddings, sample_ids)\n",
        "    #\n",
        "    # calculate similar image and cluster them based on cosine similarity\n",
        "    cosine_clusters = clusterer.cosine_similarity_clustering(threshold=0.9)\n",
        "    assign_clusters_to_fiftyone(view, cosine_clusters , field_name=\"cluster_cosine_similarity\")\n",
        "\n",
        "    #cosine_clusters = clusterer.cosine_similarity_clustering(threshold=0.9)\n",
        "\n",
        "    print( f\" number of total clusters {len(cosine_clusters)} for {label}\")\n",
        "    # Now select the data based on the rule\n",
        "    selector = ClusterDataSelector(cosine_clusters, cluster_threshold=5, test_fraction=0.3)\n",
        "    selection_result = selector.select()\n",
        "\n",
        "\n",
        "\n",
        "    print( f\"number of train images {len(selection_result['train'])}\")\n",
        "    print( f\"number of test images {len(selection_result['test'])}\")\n",
        "    print( f\"number of eliminated images {len(selection_result['eliminated'])}\")\n",
        "    print( f\"number of unknown images {len(selection_result['unknown'])}\")\n",
        "\n",
        "    assign_split_flags(view, selection_result, field_name=\"testtrainsplit_cosine_similarity\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CrY-BXPtxr2X",
        "outputId": "8d29c740-000f-463e-bdf5-fbae5cb2e085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working on Kwang\n",
            "number of images for Kwang : 242\n",
            " number of total clusters 35 for Kwang\n",
            "number of train images 227\n",
            "number of test images 10\n",
            "number of eliminated images 5\n",
            "working on Lua\n",
            "number of images for Lua : 181\n",
            " number of total clusters 12 for Lua\n",
            "number of train images 177\n",
            "number of test images 3\n",
            "number of eliminated images 1\n",
            "working on Kyyavera\n",
            "number of images for Kyyavera : 82\n",
            " number of total clusters 3 for Kyyavera\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Ague\n",
            "number of images for Ague : 6\n",
            " number of total clusters 2 for Ague\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Tomas\n",
            "number of images for Tomas : 97\n",
            " number of total clusters 7 for Tomas\n",
            "number of train images 94\n",
            "number of test images 2\n",
            "number of eliminated images 1\n",
            "working on Pixana\n",
            "number of images for Pixana : 81\n",
            " number of total clusters 9 for Pixana\n",
            "number of train images 78\n",
            "number of test images 2\n",
            "number of eliminated images 1\n",
            "working on Bagua\n",
            "number of images for Bagua : 105\n",
            " number of total clusters 13 for Bagua\n",
            "number of train images 101\n",
            "number of test images 3\n",
            "number of eliminated images 1\n",
            "working on Ariely\n",
            "number of images for Ariely : 69\n",
            " number of total clusters 8 for Ariely\n",
            "number of train images 66\n",
            "number of test images 2\n",
            "number of eliminated images 1\n",
            "working on Tingana\n",
            "number of images for Tingana : 13\n",
            " number of total clusters 2 for Tingana\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Ipepo\n",
            "number of images for Ipepo : 39\n",
            " number of total clusters 20 for Ipepo\n",
            "number of train images 33\n",
            "number of test images 6\n",
            "number of eliminated images 0\n",
            "working on Marcela\n",
            "number of images for Marcela : 318\n",
            " number of total clusters 61 for Marcela\n",
            "number of train images 294\n",
            "number of test images 18\n",
            "number of eliminated images 6\n",
            "working on Solar\n",
            "number of images for Solar : 67\n",
            " number of total clusters 11 for Solar\n",
            "number of train images 63\n",
            "number of test images 3\n",
            "number of eliminated images 1\n",
            "working on Bororo\n",
            "number of images for Bororo : 55\n",
            " number of total clusters 18 for Bororo\n",
            "number of train images 47\n",
            "number of test images 5\n",
            "number of eliminated images 3\n",
            "working on Saseka\n",
            "number of images for Saseka : 125\n",
            " number of total clusters 22 for Saseka\n",
            "number of train images 118\n",
            "number of test images 6\n",
            "number of eliminated images 1\n",
            "working on Ti\n",
            "number of images for Ti : 158\n",
            " number of total clusters 33 for Ti\n",
            "number of train images 146\n",
            "number of test images 9\n",
            "number of eliminated images 3\n",
            "working on Alira\n",
            "number of images for Alira : 48\n",
            " number of total clusters 13 for Alira\n",
            "number of train images 45\n",
            "number of test images 3\n",
            "number of eliminated images 0\n",
            "working on Tupa\n",
            "number of images for Tupa : 26\n",
            " number of total clusters 2 for Tupa\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Katniss\n",
            "number of images for Katniss : 91\n",
            " number of total clusters 5 for Katniss\n",
            "number of train images 90\n",
            "number of test images 1\n",
            "number of eliminated images 0\n",
            "working on Inka\n",
            "number of images for Inka : 45\n",
            " number of total clusters 3 for Inka\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Rio\n",
            "number of images for Rio : 9\n",
            " number of total clusters 3 for Rio\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Juru\n",
            "number of images for Juru : 4\n",
            " number of total clusters 4 for Juru\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Pyte\n",
            "number of images for Pyte : 50\n",
            " number of total clusters 6 for Pyte\n",
            "number of train images 49\n",
            "number of test images 1\n",
            "number of eliminated images 0\n",
            "working on Oxum\n",
            "number of images for Oxum : 32\n",
            " number of total clusters 6 for Oxum\n",
            "number of train images 31\n",
            "number of test images 1\n",
            "number of eliminated images 0\n",
            "working on Madalena\n",
            "number of images for Madalena : 46\n",
            " number of total clusters 10 for Madalena\n",
            "number of train images 43\n",
            "number of test images 3\n",
            "number of eliminated images 0\n",
            "working on Medrosa\n",
            "number of images for Medrosa : 322\n",
            " number of total clusters 51 for Medrosa\n",
            "number of train images 297\n",
            "number of test images 15\n",
            "number of eliminated images 10\n",
            "working on Apeiara\n",
            "number of images for Apeiara : 66\n",
            " number of total clusters 15 for Apeiara\n",
            "number of train images 62\n",
            "number of test images 4\n",
            "number of eliminated images 0\n",
            "working on Capi\n",
            "number of images for Capi : 4\n",
            " number of total clusters 2 for Capi\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Benita\n",
            "number of images for Benita : 113\n",
            " number of total clusters 9 for Benita\n",
            "number of train images 111\n",
            "number of test images 2\n",
            "number of eliminated images 0\n",
            "working on Ibaca\n",
            "number of images for Ibaca : 4\n",
            " number of total clusters 2 for Ibaca\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Abril\n",
            "number of images for Abril : 37\n",
            " number of total clusters 11 for Abril\n",
            "number of train images 34\n",
            "number of test images 3\n",
            "number of eliminated images 0\n",
            "working on Luna\n",
            "number of images for Luna : 21\n",
            " number of total clusters 3 for Luna\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Hero\n",
            "number of images for Hero : 5\n",
            " number of total clusters 3 for Hero\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Borbotetinha\n",
            "number of images for Borbotetinha : 7\n",
            " number of total clusters 1 for Borbotetinha\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Kamaikua\n",
            "number of images for Kamaikua : 165\n",
            " number of total clusters 12 for Kamaikua\n",
            "number of train images 160\n",
            "number of test images 3\n",
            "number of eliminated images 2\n",
            "working on Donal\n",
            "number of images for Donal : 13\n",
            " number of total clusters 3 for Donal\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Xando\n",
            "number of images for Xando : 5\n",
            " number of total clusters 4 for Xando\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n",
            "working on Kasimir\n",
            "number of images for Kasimir : 20\n",
            " number of total clusters 6 for Kasimir\n",
            "number of train images 19\n",
            "number of test images 1\n",
            "number of eliminated images 0\n",
            "working on Patricia\n",
            "number of images for Patricia : 26\n",
            " number of total clusters 11 for Patricia\n",
            "number of train images 23\n",
            "number of test images 3\n",
            "number of eliminated images 0\n",
            "working on Ousado\n",
            "number of images for Ousado : 311\n",
            " number of total clusters 52 for Ousado\n",
            "number of train images 291\n",
            "number of test images 15\n",
            "number of eliminated images 5\n",
            "working on Jaju\n",
            "number of images for Jaju : 166\n",
            " number of total clusters 24 for Jaju\n",
            "number of train images 158\n",
            "number of test images 7\n",
            "number of eliminated images 1\n",
            "working on Overa\n",
            "number of images for Overa : 89\n",
            " number of total clusters 15 for Overa\n",
            "number of train images 85\n",
            "number of test images 4\n",
            "number of eliminated images 0\n",
            "working on Pollyanna\n",
            "number of images for Pollyanna : 28\n",
            " number of total clusters 8 for Pollyanna\n",
            "number of train images 26\n",
            "number of test images 2\n",
            "number of eliminated images 0\n",
            "working on Estella\n",
            "number of images for Estella : 24\n",
            " number of total clusters 8 for Estella\n",
            "number of train images 22\n",
            "number of test images 2\n",
            "number of eliminated images 0\n",
            "working on Bernard\n",
            "number of images for Bernard : 23\n",
            " number of total clusters 5 for Bernard\n",
            "number of train images 21\n",
            "number of test images 1\n",
            "number of eliminated images 1\n",
            "working on Guaraci\n",
            "number of images for Guaraci : 58\n",
            " number of total clusters 5 for Guaraci\n",
            "number of train images 57\n",
            "number of test images 1\n",
            "number of eliminated images 0\n",
            "working on Akaloi\n",
            "number of images for Akaloi : 32\n",
            " number of total clusters 9 for Akaloi\n",
            "number of train images 30\n",
            "number of test images 2\n",
            "number of eliminated images 0\n",
            "working on Tango\n",
            "number of images for Tango : 19\n",
            " number of total clusters 3 for Tango\n",
            "number of train images 0\n",
            "number of test images 0\n",
            "number of eliminated images 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fo.launch_app(dataset)"
      ],
      "metadata": {
        "id": "M6b_J9oj0OJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the filtered dataset\n",
        "base_dir = project_path[user] / Path('images/cropped_body')\n",
        "storage_dir = project_path[user] / Path('datasets/version_16')\n",
        "\n",
        "dataset.export(\n",
        "    # Directory to save the datasets\n",
        "    export_dir=str(storage_dir),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    export_media=False,\n",
        "    rel_dir=base_dir\n",
        ")"
      ],
      "metadata": {
        "id": "1Q519_RN0R22",
        "outputId": "8be5373b-cded-4235-dfd4-9e43b0489b2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.data.exporters:Exporting samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |██████████████████| 3598/3598 [8.7s elapsed, 0s remaining, 710.5 docs/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |██████████████████| 3598/3598 [8.7s elapsed, 0s remaining, 710.5 docs/s]      \n"
          ]
        }
      ]
    }
  ]
}